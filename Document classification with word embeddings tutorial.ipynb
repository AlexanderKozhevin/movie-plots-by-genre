{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings for Fun and Profit\n",
    "### Document classification with Gensim\n",
    "\n",
    "In this tutorial we'll classify movie plots by genre using word embeddings techniques in [gensim](http://radimrehurek.com/gensim/) . \n",
    "\n",
    "See accompanying slides in this repo.\n",
    "\n",
    "We will show how to get a __'hello-world'__ first untuned run using 7 techniques:\n",
    "\n",
    "- Bag of words\n",
    "\n",
    "- Character n-grams\n",
    "\n",
    "- TF-IDF \n",
    "\n",
    "- Averaging word2vec vectors\n",
    "\n",
    "- doc2vec\n",
    "\n",
    "- Deep IR \n",
    "\n",
    "- Word Mover's Distance\n",
    "\n",
    "The goal of this tutorial is to show the API so you can start tuning them yourself. Model tuning of the models is out of scope of this tutorial.\n",
    "\n",
    "We will also compare the accuracy of this first 'no tuning'/out of the box run of these techniques. It is in no way an indication of their best peformance that can be achieved with proper tuning. The benefit of the comparison is to manage the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "- Python 3\n",
    "- [Google News pre-trained word2vec (1.5 GB)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "- gensim\n",
    "- sklearn\n",
    "- pandas\n",
    "- matplotlib\n",
    "- nltk with English stopwords\n",
    "- pyemd\n",
    "- 4 GB RAM\n",
    "- 8 GB disk space for WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "We will use MovieLens dataset linked with plots from OMDB. Thanks to [Sujit Pal](http://sujitpal.blogspot.de/2016/04/predicting-movie-tags-from-plots-using.html) for this linking idea. The prepared csv is in this repository. If you wish to link the datasets yourself - see the code in the [blog]((http://sujitpal.blogspot.de/2016/04/predicting-movie-tags-from-plots-using.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
    "df = df.dropna()\n",
    "df['plot'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The dataset is only __170k__ words. It is quite small but makes sure we don't have to wait a long time for the code to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "my_tags = ['sci-fi' , 'action', 'comedy', 'fantasy', 'animation', 'romance']\n",
    "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data is very unbalanced. We have Comedy as majority class. \n",
    "\n",
    "A naive classifier that predicts everything to be comedy already achieves __40%__ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The language in sci-fi plots differs a lot from action plots, so there should be some signal here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['plot', 'tag']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Genre:', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_plot(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_plot(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train/test split of 90/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model evaluation approach\n",
    "We will use confusion matrices to evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(my_tags))\n",
    "    target_names = my_tags\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict(vectorizer, classifier, data):\n",
    "    data_features = vectorizer.transform(data['plot'])\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = data['tag']\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline: bag of words, n-grams, tf-idf\n",
    "Let's start with some simple baselines before diving into more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest document feature is just a count of each word occurrence in a document.\n",
    "\n",
    "We remove stop-words and use NLTK tokenizer then limit our vocabulary to 3k most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=3000) \n",
    "train_data_features = count_vectorizer.fit_transform(train_data['plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-modal logistic regression is a simple white-box classifier. We will use either logistic regression or KNN throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()[80:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nothing impressive - only 2% better better than the classifier that thinks that everything is a comedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predict(count_vectorizer, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "White box vectorizer and classifier are great! We can see what are the most important words for sci-fi. This makes it very easy to tune and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_influential_words(vectorizer, genre_index=0, num_words=10):\n",
    "    features = vectorizer.get_feature_names()\n",
    "    max_coef = sorted(enumerate(logreg.coef_[genre_index]), key=lambda x:x[1], reverse=True)\n",
    "    return [features[x[0]] for x in max_coef[:num_words]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words for the fantasy genre\n",
    "genre_tag_id = 1\n",
    "print(my_tags[genre_tag_id])\n",
    "most_influential_words(count_vectorizer, genre_tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_data_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Character N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character _n-gram_ is a chunk of a document of length _n_. It is a poor man's tokenizer but sometimes works well. The parameter _n_ depends on language and the corpus. We choose length between 3 and 6 characters and to only focus on 3k most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_gram_vectorizer = CountVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=([2,5]),\n",
    "    tokenizer=None,    \n",
    "    preprocessor=None,                               \n",
    "    max_features=3000) \n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "train_data_features = n_gram_vectorizer.fit_transform(train_data['plot'])\n",
    "\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_gram_vectorizer.get_feature_names()[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The results are worse than using a tokenizer and bag of words. Probably due to not removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(n_gram_vectorizer, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "[Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a little more advanced way to count words in a document.\n",
    "It adjusts for document length, word frequency and most importantly for frequency of a particular word in a particular document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_vect = TfidfVectorizer(\n",
    "    min_df=2, tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english')\n",
    "train_data_features = tf_vect.fit_transform(train_data['plot'])\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, train_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf_vect.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict(tf_vect, logreg, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "White box vectorizer and classifier are great! We can see what are the most important words for sci-fi. This makes it very easy to tune and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_influential_words(tf_vect, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Things to try with bag of words\n",
    "\n",
    "10 mins for exercises.\n",
    "\n",
    "For more insight into the model print out the most influential words for a particular plot.\n",
    "\n",
    "Try n-grams with TF-IDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Averaging word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use more complex features rather than just counting words.\n",
    "\n",
    "A great recent achievement of NLP is the [word2vec embedding](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). See Chris Moody's [video](https://www.youtube.com/watch?v=vkfXBGnDplQ) for a great introduction to word2vec. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we load a word2vec model. It has been pre-trained by Google on a 100 billon word Google News corpus. You can play with this model using a fun [web-app](http://rare-technologies.com/word2vec-tutorial/#app).\n",
    "\n",
    "Link to the web-app: http://rare-technologies.com/word2vec-tutorial/#app\n",
    "\n",
    "Vocabulary size: 3 mln words. \n",
    "\n",
    "__Warning__: 3 mins to load, takes 4 GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "wv = Word2Vec.load_word2vec_format(\n",
    "    \"/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz\",\n",
    "    binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13000, 13020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a vector for each word. How do we get a vector for a sequence of words (aka a document)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most naive way is just to take an average. [Mike Tamir](https://www.youtube.com/watch?v=7gTjYwiaJiU) has suggested that the resulting vector points to a single word summarising the whole document. For example all words in a book\n",
    " ‘A tale of two cities’ should add up to 'class-struggle’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/naivedoc2vec.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For word2vec we apply a different tokenization. We want to preserve case as the vocabulary distingushes lower and upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how KNN and logistic regression classifiers perform on these word-averaging document features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_naive_dv = KNeighborsClassifier(n_neighbors=3, n_jobs=1, algorithm='brute', metric='cosine' )\n",
    "knn_naive_dv.fit(X_train_word_average, train_data.tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted = knn_naive_dv.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predicted, test_data.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "KNN is even worse than the naive 'everything is comedy' baseline! Let's see if logistic regression is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
    "predicted = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! It gives __54%__ accuracy. Best that we have seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predicted, test_data.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now just for fun let's see if text summarisation works on our data. Let's pick a plot and see which words it averages to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.iloc()[56]['plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hmm... The summarisation doesn't work here. Any ideas why? Hint: look at the area where the average ends up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[X_test_word_average[56]], restrict_vocab=100000, topn=30)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2vec things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "10 mins exercise\n",
    "\n",
    "Remove stop-words. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in stopwords.words('english'):\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What accuracy do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### More word2vec things to try\n",
    "\n",
    "Experiment with other pre-trained models - see nice [list](https://github.com/3Top/word2vec-api/) from 3Top.\n",
    "\n",
    "\n",
    "Use Gensim's GloVe converter.\n",
    "\n",
    "\n",
    "Do IDF weighting in the averaging function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) by Google suggests a model for document classification called Paragraph Vectors Doc2Vec or Doc2vec in short. It is very similar to word2vec. \n",
    "\n",
    "It introduces 'a tag' - a word that is in every context in the document.\n",
    "\n",
    "For our first try we tag every plot with its genre. This makes it 'semi-supervised' learning - the genre labels is just one objective among many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tagged = train_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['plot']), tags=[r.tag]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tagged = test_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['plot']), tags=[r.tag]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a training entry looks like - an example plot tagged by 'sci-fi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tagged.values[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainsent = train_tagged.values\n",
    "testsent = test_tagged.values\n",
    "\n",
    "# simple gensim doc2vec api\n",
    "doc2vec_model = Doc2Vec(trainsent, workers=1, size=5, iter=20, dm=1)\n",
    "\n",
    "train_targets, train_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in trainsent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interesting thing about doc2vec is that we need to run gradient descent during prediction to infer the vector for an unseen document. An unseen document is initially assigned a random vector and then this vector fit by gradient descent. Because of this randomness we get different vectors on re-runs of the next cell.\n",
    "\n",
    "Consequently, the accuracy of logistic regression changes when the test set vectors change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_targets, test_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_regressors, train_targets)\n",
    "evaluate_prediction(logreg.predict(test_regressors), test_targets, title=str(doc2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KNN gives a lower accuracy than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "knn_test_predictions = [\n",
    "    doc2vec_model.docvecs.most_similar([pred_vec], topn=1)[0][0]\n",
    "    for pred_vec in test_regressors\n",
    "]\n",
    "evaluate_prediction(knn_test_predictions, test_targets, str(doc2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2vec gives us a vector for each genre so we can see which genres are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar('action')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Words surrounding the 'sci-fi' tag describe it pretty accurately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.most_similar([doc2vec_model.docvecs['sci-fi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Doc2vec exercise\n",
    "\n",
    "10 mins\n",
    "\n",
    "Find the random seed that gives the best prediction. :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 20\n",
    "\n",
    "doc2vec_model.seed = seed\n",
    "doc2vec_model.random = random.RandomState(seed)\n",
    "\n",
    "\n",
    "test_targets, test_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])\n",
    "\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5, random_state=42)\n",
    "logreg = logreg.fit(train_regressors, train_targets)\n",
    "evaluate_prediction(logreg.predict(test_regressors), test_targets, title=str(doc2vec_model))\n",
    "print doc2vec_model.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doc2vec things to try\n",
    "Try tagging every sentence with a unique tag 'SENT_123' and then apply KNN. \n",
    "\n",
    "Try multiple tags per plot as in this repo published __today__ : https://github.com/sindbach/doc2vec_pymongo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Deep IR' is a technique developed by  [“Document Classification by Inversion of Distributed Language Representations”, Matt Taddy](http://arxiv.org/pdf/1504.07295v3.pdf). Matt has contributed a gensim [tutorial](https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb) - great source of more in depth information.\n",
    "\n",
    "In short the algorithm is:\n",
    "\n",
    "1. Train a word2vec model only on comedy plots.\n",
    "\n",
    "2. Trian another model only on sci-fi, another on romance etc. Get 6 models - one for each genre.\n",
    "\n",
    "3. Take a plot and see which model fits it best using Bayes' Theorem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tokenization is different from other methods. The reason for this is that we are following an original approach in the paper. The purpose of this tutorial is to see how the models behave out of the box.\n",
    "\n",
    "We just clean non-alphanumeric characters and split by sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contractions = re.compile(r\"'|-|\\\"\")\n",
    "# all non alphanumeric\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "# single character removal\n",
    "singles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = symbols.sub(r' \\1 ', text)\n",
    "    text = singles.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "# sentence splitter\n",
    "alteos = re.compile(r'([!\\?])')\n",
    "def sentences(l):\n",
    "    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n",
    "    return l.split(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plots(label):\n",
    "    my_df = None\n",
    "    if label=='training':\n",
    "        my_df = train_data\n",
    "    else:\n",
    "        my_df = test_data\n",
    "    for i, row in my_df.iterrows():\n",
    "        yield {'y':row['tag'],\\\n",
    "        'x':[clean(s).split() for s in sentences(row['plot'])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The corpus is small so can be read into memory\n",
    "revtrain = list(plots(\"training\"))\n",
    "revtest = list(plots(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle training set for unbiased word2vec training\n",
    "np.random.shuffle(revtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def tag_sentences(reviews, stars=my_tags):  \n",
    "    for r in reviews:\n",
    "        if r['y'] in stars:\n",
    "            for s in r['x']:\n",
    "                yield s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example `sci-fi` sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next(tag_sentences(revtrain, my_tags[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We train our own 6 word2vec models from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "## training\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=100, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0, # we only have scoring for the hierarchical softmax setup\n",
    "    \n",
    "    )\n",
    "print(basemodel)\n",
    "basemodel.build_vocab(tag_sentences(revtrain)) \n",
    "from copy import deepcopy\n",
    "genremodels = [deepcopy(basemodel) for i in range(len(my_tags))]\n",
    "for i in range(len(my_tags)):\n",
    "    slist = list(tag_sentences(revtrain, my_tags[i]))\n",
    "    print(my_tags[i], \"genre (\", len(slist), \")\")\n",
    "    genremodels[i].train(  slist, total_examples=len(slist) )\n",
    "# get the probs (note we give docprob a list of lists of words, plus the models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will compute most likely class for a plot using Bayes' Theorem formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/deep_ir_bayes.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For any new sentence we can obtain its _likelihood_ (lhd; actually, the composite likelihood approximation; see the paper) using the [score](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score) function in the `word2vec` class.  We get the likelihood for each sentence in the first test review, then convert to a probability over star ratings. Every sentence in the review is evaluated separately and the final star rating of the review is an average vote of all the sentences. This is all in the following handy wrapper. (from the original [tutorial](https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb) by Matt Taddy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## predict\n",
    "probs = docprob( [r['x'] for r in revtest], genremodels )  \n",
    "predictions = probs.idxmax(axis=1).apply(lambda x: my_tags[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tag_index = 0\n",
    "col_name = \"out-of-sample prob positive for \" + my_tags[tag_index]\n",
    "probpos = pd.DataFrame({col_name:probs[[tag_index]].sum(axis=1), \n",
    "                        \"true genres\": [r['y'] for r in revtest]})\n",
    "probpos.boxplot(col_name,by=\"true genres\", figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "target = [r['y'] for r in revtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predictions, target, \"Deep IR with word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Performance is worse than for a naive predictor that says that everything is `comedy`.\n",
    "\n",
    "### Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "It is because we train each word2vec model from scratch on a very small sample of about 30k words.\n",
    "\n",
    "This model needs more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Mover's Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/wmd_gelato.png'>\n",
    "\n",
    "Image from \n",
    "http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Word Mover's Distance is a new algorithm developed in by [Matt Kusner](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf). There is Matt's code on [github](https://github.com/mkusner/wmd) and also Gensim can compute WMD similarity in this [PR](https://github.com/piskvorky/gensim/pull/659)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For KNN the best code is from [Vlad Niculae's blog](http://vene.ro/blog/word-movers-distance-in-python.html). He is a contributor to sklearn and did great integration of WMD with sklearn KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: Write 7 GB file on disk to use memory mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __This part requires Python 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_folder = '/data/'\n",
    "\n",
    "fp = np.memmap(data_folder + \"embed.dat\", dtype=np.double, mode='w+', shape=wv.syn0norm.shape)\n",
    "fp[:] = wv.syn0norm[:]\n",
    "\n",
    "with smart_open(data_folder + \"embed.vocab\", \"w\") as f: \n",
    "    for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "        print(w.encode('utf8'), file=f)\n",
    "del fp, wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.memmap(data_folder + \"embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with smart_open(data_folder + \"embed.vocab\", mode=\"rb\") as f:\n",
    "    vocab_list = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn KNN integration with WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"%%file word_movers_knn.py\"\"\"\n",
    "\n",
    "# Authors: Vlad Niculae, Matt Kusner\n",
    "# License: Simplified BSD\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.cross_validation import check_cv\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from pyemd import emd\n",
    "\n",
    "\n",
    "class WordMoversKNN(KNeighborsClassifier):\n",
    "    \"\"\"K nearest neighbors classifier using the Word Mover's Distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    W_embed : array, shape: (vocab_size, embed_size)\n",
    "        Precomputed word embeddings between vocabulary items.\n",
    "        Row indices should correspond to the columns in the bag-of-words input.\n",
    "\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for :meth:`k_neighbors` queries.\n",
    "\n",
    "    n_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for Word Mover's Distance computation.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "    \n",
    "    verbose : int, optional\n",
    "        Controls the verbosity; the higher, the more messages. Defaults to 0.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger\n",
    "    From Word Embeddings To Document Distances\n",
    "    The International Conference on Machine Learning (ICML), 2015\n",
    "    http://mkusner.github.io/publications/WMD.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    _pairwise = False\n",
    "\n",
    "    def __init__(self, W_embed, n_neighbors=1, n_jobs=1, verbose=False):\n",
    "        self.W_embed = W_embed\n",
    "        self.verbose = verbose\n",
    "        super(WordMoversKNN, self).__init__(n_neighbors=n_neighbors, n_jobs=n_jobs,\n",
    "                                            metric='precomputed', algorithm='brute')\n",
    "\n",
    "    def _wmd(self, i, row, X_train):\n",
    "        \"\"\"Compute the WMD between training sample i and given test row.\n",
    "        \n",
    "        Assumes that `row` and train samples are sparse BOW vectors summing to 1.\n",
    "        \"\"\"\n",
    "        union_idx = np.union1d(X_train[i].indices, row.indices)\n",
    "        W_minimal = self.W_embed[union_idx]\n",
    "        W_dist = euclidean_distances(W_minimal)\n",
    "        bow_i = X_train[i, union_idx].A.ravel()\n",
    "        bow_j = row[:, union_idx].A.ravel()\n",
    "        return emd(bow_i, bow_j, W_dist)\n",
    "    \n",
    "    def _wmd_row(self, row, X_train):\n",
    "        \"\"\"Wrapper to compute the WMD of a row with all training samples.\n",
    "        \n",
    "        Assumes that `row` and train samples are sparse BOW vectors summing to 1.\n",
    "        Useful for parallelization.\n",
    "        \"\"\"\n",
    "        n_samples_train = X_train.shape[0]\n",
    "        return [self._wmd(i, row, X_train) for i in range(n_samples_train)]\n",
    "\n",
    "    def _pairwise_wmd(self, X_test, X_train=None):\n",
    "        \"\"\"Computes the word mover's distance between all train and test points.\n",
    "        \n",
    "        Parallelized over rows of X_test.\n",
    "        \n",
    "        Assumes that train and test samples are sparse BOW vectors summing to 1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test: scipy.sparse matrix, shape: (n_test_samples, vocab_size)\n",
    "            Test samples.\n",
    "        \n",
    "        X_train: scipy.sparse matrix, shape: (n_train_samples, vocab_size)\n",
    "            Training samples. If `None`, uses the samples the estimator was fit with.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dist : array, shape: (n_test_samples, n_train_samples)\n",
    "            Distances between all test samples and all train samples.\n",
    "        \n",
    "        \"\"\"\n",
    "        n_samples_test = X_test.shape[0]\n",
    "        \n",
    "        if X_train is None:\n",
    "            X_train = self._fit_X\n",
    "\n",
    "        dist = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n",
    "            delayed(self._wmd_row)(test_sample, X_train)\n",
    "            for test_sample in X_test)\n",
    "\n",
    "        return np.array(dist)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using X as training data and y as target values\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse matrix, shape: (n_samples, n_features)\n",
    "            Training data. \n",
    "\n",
    "        y : {array-like, sparse matrix}\n",
    "            Target values of shape = [n_samples] or [n_samples, n_outputs]\n",
    "\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "        return super(WordMoversKNN, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the provided data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy.sparse matrix, shape (n_test_samples, vocab_size)\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape [n_samples]\n",
    "            Class labels for each data sample.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "        dist = self._pairwise_wmd(X)\n",
    "        return super(WordMoversKNN, self).predict(dist)\n",
    "    \n",
    "    \n",
    "class WordMoversKNNCV(WordMoversKNN):\n",
    "    \"\"\"Cross-validated KNN classifier using the Word Mover's Distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W_embed : array, shape: (vocab_size, embed_size)\n",
    "        Precomputed word embeddings between vocabulary items.\n",
    "        Row indices should correspond to the columns in the bag-of-words input.\n",
    "\n",
    "    n_neighbors_try : sequence, optional\n",
    "        List of ``n_neighbors`` values to try.\n",
    "        If None, tries 1-5 neighbors.\n",
    "\n",
    "    scoring : string, callable or None, optional, default: None\n",
    "        A string (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "        For integer/None inputs, StratifiedKFold is used.\n",
    "\n",
    "    n_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for Word Mover's Distance computation.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "\n",
    "    verbose : int, optional\n",
    "        Controls the verbosity; the higher, the more messages. Defaults to 0.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cv_scores_ : array, shape (n_folds, len(n_neighbors_try))\n",
    "        Test set scores for each fold.\n",
    "\n",
    "    n_neighbors_ : int,\n",
    "        The best `n_neighbors` value found.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger\n",
    "    From Word Embeddings To Document Distances\n",
    "    The International Conference on Machine Learning (ICML), 2015\n",
    "    http://mkusner.github.io/publications/WMD.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, W_embed, n_neighbors_try=None, scoring=None, cv=3,\n",
    "                 n_jobs=1, verbose=False):\n",
    "        self.cv = cv\n",
    "        self.n_neighbors_try = n_neighbors_try\n",
    "        self.scoring = scoring\n",
    "        super(WordMoversKNNCV, self).__init__(W_embed,\n",
    "                                              n_neighbors=None,\n",
    "                                              n_jobs=n_jobs,\n",
    "                                              verbose=verbose)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit KNN model by choosing the best `n_neighbors`.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : scipy.sparse matrix, (n_samples, vocab_size)\n",
    "            Data\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target\n",
    "        \"\"\"\n",
    "        if self.n_neighbors_try is None:\n",
    "            n_neighbors_try = range(1, 6)\n",
    "        else:\n",
    "            n_neighbors_try = self.n_neighbors_try\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "\n",
    "        cv = check_cv(self.cv, X, y)\n",
    "        knn = KNeighborsClassifier(metric='precomputed', algorithm='brute')\n",
    "        scorer = check_scoring(knn, scoring=self.scoring)\n",
    "\n",
    "        scores = []\n",
    "        for train_ix, test_ix in cv:\n",
    "            dist = self._pairwise_wmd(X[test_ix], X[train_ix])\n",
    "            knn.fit(X[train_ix], y[train_ix])\n",
    "            scores.append([\n",
    "                scorer(knn.set_params(n_neighbors=k), dist, y[test_ix])\n",
    "                for k in n_neighbors_try\n",
    "            ])\n",
    "        scores = np.array(scores)\n",
    "        self.cv_scores_ = scores\n",
    "\n",
    "        best_k_ix = np.argmax(np.mean(scores, axis=0))\n",
    "        best_k = n_neighbors_try[best_k_ix]\n",
    "        self.n_neighbors = self.n_neighbors_ = best_k\n",
    "\n",
    "        return super(WordMoversKNNCV, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "\n",
    "flat_train_tokenized = [item for sublist in train_tokenized for item in sublist]\n",
    "flat_test_tokenized = [item for sublist in test_tokenized for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up performance we focus only on the words that are both in Google News model and in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the word2vec model was loaded with strings as byte-arrays so need to convert\n",
    "def convert_to_vocab_bytes(s):\n",
    "     return bytes(\"b'\" + s + \"'\", encoding='utf-8')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(flat_train_tokenized)\n",
    "common = [word for word in vect.get_feature_names() if convert_to_vocab_bytes(word) in vocab_dict]\n",
    "W_common = W[[vocab_dict[ convert_to_vocab_bytes(w)] for w in common]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mass of the pile of mud in the Word Mover's Distance is determined by how many times a word appears in a document. So we need a CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(vocabulary=common, dtype=np.double)\n",
    "X_train = vect.fit_transform(train_tokenized)\n",
    "X_test = vect.transform(test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = WordMoversKNN(n_neighbors=1,W_embed=W_common, verbose=5, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, train_data['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: 10 minutes runtime on 7 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "predicted = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 2% above the naive baseline unfortunately. WMD achieves good results on sentiment analysis in the published paper. Maybe it works better for sentiment than for topic classification that we use it here. Or maybe preprocessing can be tuned here. It is hard to debug a black box method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(predicted, test_data['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we shown how to run 'hello-world' in 7 different document classification techniques. It is just a beginning of exploration of their features... There are a lot of parameters that can be tuned to get the best possible results out of them. The 'hello-world' run is in no way an indication of their best peformance. The goal of this tutorial is to show the API so you can start tuning them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box \"no tuning\" accuracy of bag of words is not far behind more advanced techniques. \n",
    "Tune them and the pre-processing for them well first and only then reach for more advanced methods if more accuracy is absolutely needed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
